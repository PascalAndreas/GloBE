# @package _global_
model:
  name: "Qwen1.5-MoE-A2.7B"
  
  # Model dimensions (read from weights)
  d: 2048  # hidden dimension
  p: 5632  # FFN intermediate dimension
  
  # MoE configuration
  layers_moe: 24  # number of MoE layers
  n_routed: 60    # number of routed experts per layer
  n_shared: 4     # number of shared experts per layer
  topk: 4         # number of experts selected per token
  
  # Model paths
  model_path: null  # Path to model weights, set at runtime
  cache_dir: null   # Cache directory for downloaded models
  
  # Loading configuration
  torch_dtype: "auto"  # or "float16", "bfloat16", "float32"
  device_map: "auto"
