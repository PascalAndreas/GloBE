# @package _global_
train:
  # Training steps
  steps: 10000
  
  # Batch configuration
  batch_size: 32      # number of experts per batch
  experts_per_step: 8 # experts to sample per training step
  
  # Learning rate
  lr: 1.0e-3
  lr_schedule: "cosine"  # or "constant", "linear"
  warmup_steps: 1000
  
  # Optimizer (Adam)
  adam:
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
    weight_decay: 1.0e-2
  
  # Gradient clipping
  grad_clip: 1.0
  
  # Normalization
  zscore: true  # apply z-score normalization per family
  
  # Checkpointing
  checkpoint_interval: 1000
  save_best: true
  
  # Validation
  val_interval: 500
  val_experts: 100  # number of experts to validate on
